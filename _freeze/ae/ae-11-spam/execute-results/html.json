{
  "hash": "5bd139de379155b26fbb3943bd3ef48c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Spam email\"\ncategories: \n  - Application exercise\n  - Suggested answers\neditor: visual\neditor_options: \n  chunk_output_type: console\nfig-width: 6\nfig-asp: 0.618\n---\n\n\nIn this application exercise, we'll build a spam filter.\nOr, at least, learn a bit about how spam filters are built by building a very simple (likely not very effective) one.\n\n# Goals\n\n-   Understand logistic regression as a linear model of binary outcomes\n\n-   Fit and interpret logistic regression models in R\n\n# Packages and data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(ggthemes)\n\nhp_spam <- read_csv(\"data/hp-spam.csv\")\n```\n:::\n\n\nTo illustrate logistic regression, we will build a spam filter from email data.\nToday's data consists of 4601 emails that are classified as spam or non-spam.\n\n-   `type = 1` is spam\n-   `type = 0` is non-spam\n\nThe data was collected at Hewlett-Packard labs and contains 4601 variables.\nThe first 48 variables are specific keywords and each observation is the percentage of appearance of that word in the message.\nClick [here](https://rdrr.io/cran/kernlab/man/spam.html) to read more.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(hp_spam)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 4,601\nColumns: 58\n$ make              <dbl> 0.00, 0.21, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ address           <dbl> 0.64, 0.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ all               <dbl> 0.64, 0.50, 0.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.46…\n$ num3d             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ our               <dbl> 0.32, 0.14, 1.23, 0.63, 0.63, 1.85, 1.92, 1.88, 0.61…\n$ over              <dbl> 0.00, 0.28, 0.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ remove            <dbl> 0.00, 0.21, 0.19, 0.31, 0.31, 0.00, 0.00, 0.00, 0.30…\n$ internet          <dbl> 0.00, 0.07, 0.12, 0.63, 0.63, 1.85, 0.00, 1.88, 0.00…\n$ order             <dbl> 0.00, 0.00, 0.64, 0.31, 0.31, 0.00, 0.00, 0.00, 0.92…\n$ mail              <dbl> 0.00, 0.94, 0.25, 0.63, 0.63, 0.00, 0.64, 0.00, 0.76…\n$ receive           <dbl> 0.00, 0.21, 0.38, 0.31, 0.31, 0.00, 0.96, 0.00, 0.76…\n$ will              <dbl> 0.64, 0.79, 0.45, 0.31, 0.31, 0.00, 1.28, 0.00, 0.92…\n$ people            <dbl> 0.00, 0.65, 0.12, 0.31, 0.31, 0.00, 0.00, 0.00, 0.00…\n$ report            <dbl> 0.00, 0.21, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ addresses         <dbl> 0.00, 0.14, 1.75, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ free              <dbl> 0.32, 0.14, 0.06, 0.31, 0.31, 0.00, 0.96, 0.00, 0.00…\n$ business          <dbl> 0.00, 0.07, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ email             <dbl> 1.29, 0.28, 1.03, 0.00, 0.00, 0.00, 0.32, 0.00, 0.15…\n$ you               <dbl> 1.93, 3.47, 1.36, 3.18, 3.18, 0.00, 3.85, 0.00, 1.23…\n$ credit            <dbl> 0.00, 0.00, 0.32, 0.00, 0.00, 0.00, 0.00, 0.00, 3.53…\n$ your              <dbl> 0.96, 1.59, 0.51, 0.31, 0.31, 0.00, 0.64, 0.00, 2.00…\n$ font              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num000            <dbl> 0.00, 0.43, 1.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ money             <dbl> 0.00, 0.43, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ hp                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hpl               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ george            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num650            <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ lab               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ labs              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ telnet            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num857            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ data              <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.15…\n$ num415            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ num85             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ technology        <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ num1999           <dbl> 0.00, 0.07, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ parts             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ pm                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ direct            <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ cs                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ meeting           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ original          <dbl> 0.00, 0.00, 0.12, 0.00, 0.00, 0.00, 0.00, 0.00, 0.30…\n$ project           <dbl> 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ re                <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ edu               <dbl> 0.00, 0.00, 0.06, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00…\n$ table             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ conference        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ charSemicolon     <dbl> 0.000, 0.000, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charRoundbracket  <dbl> 0.000, 0.132, 0.143, 0.137, 0.135, 0.223, 0.054, 0.2…\n$ charSquarebracket <dbl> 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ charExclamation   <dbl> 0.778, 0.372, 0.276, 0.137, 0.135, 0.000, 0.164, 0.0…\n$ charDollar        <dbl> 0.000, 0.180, 0.184, 0.000, 0.000, 0.000, 0.054, 0.0…\n$ charHash          <dbl> 0.000, 0.048, 0.010, 0.000, 0.000, 0.000, 0.000, 0.0…\n$ capitalAve        <dbl> 3.756, 5.114, 9.821, 3.537, 3.537, 3.000, 1.671, 2.4…\n$ capitalLong       <dbl> 61, 101, 485, 40, 40, 15, 4, 11, 445, 43, 6, 11, 61,…\n$ capitalTotal      <dbl> 278, 1028, 2259, 191, 191, 54, 112, 49, 1257, 749, 2…\n$ type              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n```\n\n\n:::\n:::\n\n\nThe basic logic of our model is that the frequency of certain words can help us determine whether or not an email is spam.\n\nFor example, these emails came from George's inbox.\nIf the word \"george\" (`george`) is not present in the message and the dollar symbol (`charDollar`) is, you might expect the email to be spam.\n\nUsing this data, we want to build a model that **predicts** whether a new email is spam or not.\nHow do we build a model that can do this?\n\n# Setting default theme\n\nYou can apply the same theme for your ggplots throughout your document.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_minimal())\n```\n:::\n\n\n# Building intuition\n\n## Exercise 1\n\nOne predictor model: Visualize a **linear model** where the outcome is `type` (spam or not) and `george` is the only predictor.\nThen, discuss your visualization with your neighbor.\nIs this a good model?\nWhy or why not?\n\n*Add response here.*\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n## Exercise 2\n\nWhat type of data is `type`, our outcome variable?\nWhat type should it be?\nMake any adjustments necessary and recreate the visualization from Exercise 1.\nDoes this model look better?\nWhy or why not?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\nMoral of the story so far: A linear model is not a good model for binary outcomes.\n\n# Spending your data\n\nOur ultimate goal is to do prediction (or classification) on new data -- a new email.\nTherefore, we should build a model using some of the data and then test it on the rest of the data.\n\nLet's divide the data into a training set and testing set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(109)\nhp_spam_split <- initial_split(hp_spam)\nhp_spam_train <- training(hp_spam_split)\nhp_spam_test <- testing(hp_spam_split)\n```\n:::\n\n\n## Exercise 3\n\nInspect `hp_spam_split`.\nHow many emails are in `hp_spam_train`, how many are in `hp_spam_test`.\nCheck out the documentation for the `initial_split()` function, what ratio does it use for splitting the dataset into training and testing samples?\n\n*Add response here.*\n\n## Exercise 4\n\nTwo predictor model: In this exercise focus on two predictors: `you` and `capitalTotal`.\n\n-   Create a visualization with `you` on the x-axis and `capitalTotal` on the y-axis. Color data points by whether or not they are spam (`type`). Make sure that `type` is being used as a categorical variable (factor).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n-   Fit the model predicting `type` from `you` and `capitalTotal`. Comment on how the code differs from code used in previous models we fit. Also comment on how it's similar.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n## Exercise 5\n\nWrite the model equation.\n\n$$\nlogit(\\hat{p}) = -1.50 + 0.361 \\times you + 0.00173 \\times capitalTotal\n$$\n\n$$\nlog(\\frac{\\hat{p}}{1 - \\hat{p}}) = -1.50 + 0.361 \\times you + 0.00173 \\times capitalTotal\n$$\n\n## Exercise 6\n\nWhat is the probability the email is spam if the frequency of `you` is 5% in the email and there are 2500 capital letters.\n\n-   First, do this \"by hand\" (using R as a calculator) and the model you wrote in the previous exercise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n-   Then, do it using R functions designed for prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n# Classify a new email\n\nRead a new email and figure out values of `you` and `capitalTotal` and store these in a new tibble called `new_email`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n## Exercise 8\n\nUsing your model, predict whether this email will be classified as spam or not.\nWhat does the model predict for the **probability** that this email is spam?\nWith a decision boundary of 0.5, how does the model classify thie email?\nDo you believe this classification?\nWhy or why not?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n*Add response here*\n\n# Assessing predictive ability\n\nFirst, let's start over with a new model.\n\n## Exercise 9\n\nTrain your model on the training set.\nBuild a predictive model using any combination of predictors to predict `type`.\nSave your fitted model as `my_model_fit` and display its tidy summary.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n## Exercise 10\n\nMake predictions for your testing set and augment your testing set with these predictions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n## Exercise 11\n\nWhat are the false positive and false negative rates of this model?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n\n\n# Visualizing logistic regression\n\nJust because there's greater than 50% probability an email is spam doesn't mean we have to label it as such.\nWe can adjust our **threshold** or **critical probability**, a.k.a. **decision boundary** to be more or less sensitive to spam emails.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_model_aug <- augment(my_model_fit, new_data = hp_spam_test)\ndecision_boundary <- 0.5\n\nggplot(my_model_aug, aes(x = .pred_1, y = type)) +\n  geom_jitter(alpha = 0.5, color = \"darkgray\", height = 0.2) +\n  geom_vline(xintercept = decision_boundary, color = \"red\", linetype = \"dashed\")\n```\n:::\n\n\nIn other words, we get to select a number $p^*$ such that if $p > p^*$, then label the email as spam.\n\n## Exercise 12\n\n-   What would you set your decision boundary to and why?\n\n-   Change `decision_boundary` in the code above to 0.01 and 0.999999.\n    Do the results surprise you?\n    Why or why not?\n\n*Add response here.*\n\n## Exercise 13\n\nIf you set a lower decision boundary, do you label fewer or more emails as spam?\nWhat happens if you set 0 as your boundary?\nWhat about 1 as your boundary?\nIf you very much dislike spam, should you set a high or low boundary?\n\n*Add response here.*\n\n## Exercise 14\n\nRecreate the jittered scatterplot from earlier, this time using color to indicate the following four decision regions and label them on the plot.\nMake `decision_boundary` a value you can update and re-run the code to see how the decision regions change.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add code here\n```\n:::\n",
    "supporting": [
      "ae-11-spam_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}