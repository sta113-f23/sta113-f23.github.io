{
  "hash": "936f538b4cfb0e173a94d34cea140e46",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Visual inference\"\nsubtitle: Lecture 16\nformat: \n  revealjs:\n    fig-align: center\n    fig-width: 5\n    fig-asp: 0.618\n---\n\n\n\n# Warm-up\n\n## Announcements\n\n-   Project 2 proposal feedback delayed, will have it for you by the end of tonight!\n-   Who is in class on Tuesday?\n\n## Setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tune         1.1.2\n✔ infer        1.0.5     ✔ workflows    1.1.3\n✔ modeldata    1.2.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.1     ✔ yardstick    1.2.0\n✔ recipes      1.0.8     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(openintro)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nAttaching package: 'openintro'\n\nThe following object is masked from 'package:modeldata':\n\n    ames\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(nullabor)  # for visual inference\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'nullabor'\n\nThe following object is masked from 'package:dials':\n\n    sample_size\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(skimr)     # for skimming data \n```\n:::\n\n\n\n# Now you see me, now you don't\n\n## \n\n::: task\nOne of the pictures is a photograph of a painting by Piet Mondrian while the other is a photograph of a drawing made by an IBM 7094 digital computer.\nWhich of the two do you think was done by the computer?\n:::\n\n![](images/mondrian-computer.png)\n\n::: small\nA. M. Noll, \"Human or machine: A subjective comparison of piet mondrian's \"composition with lines\" (1917) and a computer- generated picture,\" The Psychological Record, vol.\n16, pp. 1–10, 1966.\n:::\n\n## Aside: Who is Piet Mondrian?\n\n![](images/mondrian-composition-red-blue-yellow.jpg){fig-align=\"center\" width=\"800\"}\n\n## Apophenia\n\n::: task\nWhat is **apophenia**?\n:::\n\n. . .\n\n::: columns\n::: {.column width=\"40%\"}\nThe tendency to perceive a connection or meaningful pattern between unrelated or random things.\n:::\n\n::: {.column width=\"60%\"}\n![](images/cloud-witch.jpg){fig-align=\"right\" width=\"300\"}\n:::\n:::\n\n## Visualization, statistical inference, visual inference {.smaller}\n\n::: incremental\n-   **Visualization** provides tools to uncover new relationships, tools of curiosity, and much of visualization research focuses on making the chance of finding relationships as high as possible\n-   **Statistical inference** provides tools to check whether a relationship really exists (tools of skepticism) and most statistics research focuses on making sure to minimize the chance of finding a relationship that does not exist\n    -   Testing: *Is there a difference?*\n    -   Estimation: *How big is the difference?*\n-   **Visual inference** bridges these two conflicting drives to provide a tool for skepticism that can be applied in a curiosity-driven context - Testing: *Is what we see really there, i.e., is what we see in a plot of the sample an accurate reflection of the entire population?*\n:::\n\n## Hypothesis testing as a court trial\n\n-   **Null hypothesis**, $H_0$: Defendant is innocent\n\n-   **Alternative hypothesis**, $H_A$: Defendant is guilty\n\n-   **Present the evidence:** Collect data\n\n. . .\n\n-   **Judge the evidence:** \"Could these data plausibly have happened by chance if the null hypothesis were true?\"\n    -   Yes: Fail to reject $H_0$\n    -   No: Reject $H_0$\n\n## Hypothesis testing framework\n\n-   Start with a null hypothesis, $H_0$, that represents the status quo\n\n-   Set an alternative hypothesis, $H_A$, that represents the research question, i.e. what we’re testing for\n\n-   Conduct a hypothesis test under the assumption that the null hypothesis is true and calculate a **p-value** (probability of observed or more extreme outcome given that the null hypothesis is true)\n\n    -   if the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\n    -   if they do, then reject the null hypothesis in favor of the alternative\n\n## Potential errors\n\n-   Type 1 error (false negative): Acquit / reject $H_0$ when you shouldn't\n\n-   Type 2 error (false positive): Falsely convict an innocent / fail to reject $H_0$ when you shouldn't\n\n-   Costs of these errors vary based on the severity of the consequences\n\n## Statistical vs. visual inference\n\n::: columns\n::: {.column width=\"40%\"}\nStatistical\n\n-   Test statistic\n-   p-value compared to significance level\n:::\n\n::: {.column width=\"60%\"}\nVisual\n\n-   Plot of the data\n-   Humans' picking an \"innocent\" out of a series of **null plot**s\n:::\n:::\n\n# Visual inference with a lineup\n\n## The lineup protocol\n\n::: incremental\n-   Plot of the real data is randomly embedded amongst a set of null plots\n\n-   Matrix of plots is known as a **lineup**\n\n-   Null plots are generated by a method consistent with the null hypothesis\n\n-   The lineup is shown to an observer.\n    If the observer can pick the real data as different from the others, this puts weight on the statistical significance of the structure in the plot.\n\n-   The `lineup()` function from the [**nullabor**](http://dicook.github.io/nullabor/) package returns a set of generated null datasets and the real data embedded randomly among these null datasets\n:::\n\n## Using `nullabor::lineup()`\n\n-   Generating null plots:\n    -   Option 1: Provide a method of generation and let the `lineup()` function generate them\n    -   Option 2: Generating the null datasets yourself and pass them to the `lineup()` function\n\n. . .\n\n-   Position of the real dataset is hidden by default\n    -   Decrypt to find out which plot is the real dataset\n\n## Will the real `mtcars` please stand up?\n\n::: columns\n::: {.column width=\"80%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-visual-inference_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"20%\"}\n\n\n```{=html}\n<iframe src=\"https://app.sli.do/event/iJykXLe5CLzhorAiQhjnrg/embed/polls/98b5ff30-7049-4af9-ab7d-f4cd98070cd5\" width=\"300\" height=\"400\"></iframe>\n```\n\n\n:::\n:::\n\n## The making of the lineup I\n\nStep 1.\nPermute the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20211029)\nmtcars_permuted_lineup <- lineup(null_permute(\"mpg\"), mtcars)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ndecrypt(\"K4Ul YG3G R8 oSJR3RS8 g6\")\n```\n\n\n:::\n:::\n\n\n\n## The making of the lineup II\n\nStep 2.\nPeek at the permuted data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(mtcars_permuted_lineup)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      mpg cyl disp  hp drat    wt  qsec vs am gear carb .sample\n...1 10.4   6  160 110 3.90 2.620 16.46  0  1    4    4       1\n...2 21.4   6  160 110 3.90 2.875 17.02  0  1    4    4       1\n...3 14.7   4  108  93 3.85 2.320 18.61  1  1    4    1       1\n...4 15.5   6  258 110 3.08 3.215 19.44  1  0    3    1       1\n...5 30.4   8  360 175 3.15 3.440 17.02  0  0    3    2       1\n...6 15.0   6  225 105 2.76 3.460 20.22  1  0    3    1       1\n```\n\n\n:::\n:::\n\n\n\n## The making of the lineup II\n\nStep 2.\nPeek at the permuted data\n\n`n = 20` by default\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars_permuted_lineup |>\n  count(.sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   .sample  n\n1        1 32\n2        2 32\n3        3 32\n4        4 32\n5        5 32\n6        6 32\n7        7 32\n8        8 32\n9        9 32\n10      10 32\n11      11 32\n12      12 32\n13      13 32\n14      14 32\n15      15 32\n16      16 32\n17      17 32\n18      18 32\n19      19 32\n20      20 32\n```\n\n\n:::\n:::\n\n\n\n## The making of the lineup III\n\nStep 3.\nPlot the permutations\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = mtcars_permuted_lineup, aes(x = mpg, y = wt)) +\n  geom_point() +\n  facet_wrap(~ .sample)\n```\n\n::: {.cell-output-display}\n![](16-visual-inference_files/figure-html/unnamed-chunk-6-1.png){width=100%}\n:::\n:::\n\n\n\n## Decrypt the lineup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndecrypt(\"K4Ul YG3G R8 oSJR3RS8 g6\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"True data in position  16\"\n```\n\n\n:::\n:::\n\n\n\n# Visual inference with a Rorschach\n\n## The Rorschach protocol\n\n-   The **Rorschach** protocol is used to calibrate the eyes for variation due to sampling\n-   Plots generated corresponds to the null datasets, data that is consistent with a null hypothesis\n-   `rorschach()` function returns a set of null plots which are shown to observers to calibrate their eyes with variation\n\n## Using `nullabor::rorschach()`\n\n-   Generating null plots: Provide a `method` of generation and let the `rorschach()` function generate them\n\n-   Provide the `true` data set\n\n-   Set `n`, total number of samples to generate (`n = 20` by default)\n\n-   Set `p`, probability of including true data with null data (`p = 0` by default)\n\n## Train your eyes to spot the real `mtcars`\n\n::: columns\n::: {.column width=\"80%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-visual-inference_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"20%\"}\n\n\n```{=html}\n<iframe src=\"https://app.sli.do/event/iJykXLe5CLzhorAiQhjnrg/embed/polls/98b5ff30-7049-4af9-ab7d-f4cd98070cd5\" width=\"300\" height=\"400\"></iframe>\n```\n\n\n:::\n:::\n\n## The making of the rorschach I\n\nStep 1.\nPermute the data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20211029)\nmtcars_permuted_rorschach <- rorschach(\n  null_permute(\"mpg\"), mtcars\n)\n```\n:::\n\n\n\n## The making of the rorschach II\n\nStep 2.\nPeek at the permuted data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(mtcars_permuted_rorschach)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   mpg cyl disp  hp drat    wt  qsec vs am gear carb .sample\n1 10.4   6  160 110 3.90 2.620 16.46  0  1    4    4       1\n2 21.4   6  160 110 3.90 2.875 17.02  0  1    4    4       1\n3 14.7   4  108  93 3.85 2.320 18.61  1  1    4    1       1\n4 15.5   6  258 110 3.08 3.215 19.44  1  0    3    1       1\n5 30.4   8  360 175 3.15 3.440 17.02  0  0    3    2       1\n6 15.0   6  225 105 2.76 3.460 20.22  1  0    3    1       1\n```\n\n\n:::\n:::\n\n\n\n## The making of the rorschach II\n\nStep 2.\nPeek at the permuted data\n\n`n = 20` by default\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars_permuted_rorschach |>\n  count(.sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   .sample  n\n1        1 32\n2        2 32\n3        3 32\n4        4 32\n5        5 32\n6        6 32\n7        7 32\n8        8 32\n9        9 32\n10      10 32\n11      11 32\n12      12 32\n13      13 32\n14      14 32\n15      15 32\n16      16 32\n17      17 32\n18      18 32\n19      19 32\n20      20 32\n```\n\n\n:::\n:::\n\n\n\n## The making of the rorschach III\n\nStep 3.\nPlot the permutations\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = mtcars_permuted_rorschach, aes(x = mpg, y = wt)) +\n  geom_point() +\n  facet_wrap(~ .sample)\n```\n\n::: {.cell-output-display}\n![](16-visual-inference_files/figure-html/unnamed-chunk-12-1.png){width=100%}\n:::\n:::\n\n\n\n## Decrypt the rorschach\n\n-   In this particular case there's nothing to decrypt since `p` (probability of including true data with null data) is set to 0\n\n-   If `p` is higher than 0, and the true null is included, you get the decryption key\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nmtcars_permuted_rorschach <- rorschach(null_permute(\"mpg\"), mtcars, p = 0.5)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nK4Ul YG3G R8 oSJR3RS8 gI\n```\n\n\n:::\n\n```{.r .cell-code}\ndecrypt(\"K4Ul YG3G R8 oSJR3RS8 gI\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"True data in position  15\"\n```\n\n\n:::\n:::\n\n\n\n# Generating the null data\n\n## Generating the null data\n\n-   By permuting a variable (what we've done so far with `mtcars`)\n-   With a specific distribution\n-   With null residuals from a model\n-   With null data outside of **nullabor** (we won't get into this, but see [here](http://dicook.github.io/nullabor/articles/nullabor.html#generate-null-data-outside-of-nullabor) for more)\n\n## Generate null data with a specific distribution {.smaller}\n\n::: incremental\n-   The `null_dist()` function takes as input a variable name of the data and a particular distribution\n-   This variable in the data is substituted by random generations of the particular distribution\n-   The different distributions include beta, cauchy, chi-squared, exponential, f, gamma, geometric, log-normal, lognormal, logistic, negative binomial, normal, poisson, t, and weibull\n-   Parameters of the distributions are estimated from the given data (default) or can be provided as a list\n-   `null_dist()` returns a function that generates a null data set given the data\n:::\n\n## Case study: Heights of adults\n\n::: task\nThe following histogram shows the distribution of heights of 507 physically active individuals (`openintro::bdims$hgt`).\nDo the heights of these individuals follow a normal distribution?\n:::\n\n::: columns\n::: {.column width=\"80%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-visual-inference_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=90%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"20%\"}\n\n\n```{=html}\n<iframe src=\"https://app.sli.do/event/iJykXLe5CLzhorAiQhjnrg/embed/polls/2636081d-aad9-449f-aa77-82366fb74e5f\" width=\"300\" height=\"400\"></iframe>\n```\n\n\n:::\n:::\n\n## Spot the real data\n\n::: task\nWhich of the following is the plot of the real data?\n(Note: A different binwidth than the previous plot is used.)\n:::\n\n::: columns\n::: {.column width=\"80%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-visual-inference_files/figure-html/generate-null-dist-1.png){fig-align='center' width=100%}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"20%\"}\n\n\n```{=html}\n<iframe src=\"https://app.sli.do/event/iJykXLe5CLzhorAiQhjnrg/embed/polls/98b5ff30-7049-4af9-ab7d-f4cd98070cd5\" width=\"300\" height=\"400\"></iframe>\n```\n\n\n:::\n:::\n\n## Code: Generate null data with a specific distribution\n\nGenerate the null distribution\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20211029)\n\nbdims_permuted_lineup <- lineup(null_dist(\"hgt\", dist = \"normal\"), n = 10, true = bdims)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ndecrypt(\"K4Ul YG3G R8 oSJR3RS8 Ng\")\n```\n\n\n:::\n:::\n\n\n\n## Code: Decrypt\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndecrypt(\"K4Ul YG3G R8 oSJR3RS8 Ng\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"True data in position  2\"\n```\n\n\n:::\n:::\n\n\n\n## Generating the null data with residuals from a model\n\n-   `null_lm()` takes as input a model specification formula as defined by `lm()` and method for generating null residuals from the model\n\n-   Three built in methods for different (and valid) methods to generate null data when fitting a linear model:\n\n    -   `method = \"pboot\"`\n    -   `method = \"boot\"`\n    -   `method = \"rotate\"`\n\n-   `null_lm()` returns a function which given the data generates a null dataset\n\n## Case study: Black cherry trees {.smaller}\n\nData measures the diameter, height and volume of timber in 31 felled black cherry trees (`datasets::trees`)\n\n**Summary**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nskim(datasets::trees)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n── Data Summary ────────────────────────\n                           Values         \nName                       datasets::trees\nNumber of rows             31             \nNumber of columns          3              \n_______________________                   \nColumn type frequency:                    \n  numeric                  3              \n________________________                  \nGroup variables            None           \n\n── Variable type: numeric ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean    sd   p0  p25  p50  p75 p100 hist \n1 Girth                 0             1 13.2  3.14  8.3 11.0 12.9 15.2 20.6 ▃▇▃▅▁\n2 Height                0             1 76    6.37 63   72   76   80   87   ▃▃▆▇▃\n3 Volume                0             1 30.2 16.4  10.2 19.4 24.2 37.3 77   ▇▅▁▂▁\n```\n\n\n:::\n:::\n\n\n\n## Case study: Black cherry trees\n\n**Plot**\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](16-visual-inference_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](16-visual-inference_files/figure-html/unnamed-chunk-20-2.png){width=672}\n:::\n:::\n\n\n\n## Fit the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrees_fit <- lm(log(Volume) ~ log(Girth) + log(Height), \n                data = datasets::trees)\n\ntidy(trees_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    -6.63    0.800      -8.29 5.06e- 9\n2 log(Girth)      1.98    0.0750     26.4  2.42e-21\n3 log(Height)     1.12    0.204       5.46 7.81e- 6\n```\n\n\n:::\n:::\n\n\n\n## Augment the model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrees_aug <- as_tibble(datasets::trees) |>\n  mutate(\n    .resid = residuals(trees_fit),\n    .fitted = fitted(trees_fit)\n  )\n\ntrees_aug\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 31 × 5\n   Girth Height Volume   .resid .fitted\n   <dbl>  <dbl>  <dbl>    <dbl>   <dbl>\n 1   8.3     70   10.3  0.0219     2.31\n 2   8.6     65   10.3  0.0343     2.30\n 3   8.8     63   10.2  0.0138     2.31\n 4  10.5     72   16.4 -0.0106     2.81\n 5  10.7     81   18.8 -0.0430     2.98\n 6  10.8     83   19.7 -0.0420     3.02\n 7  11       66   15.6 -0.0557     2.80\n 8  11       75   18.2 -0.0443     2.95\n 9  11.1     80   22.6  0.0822     3.04\n10  11.2     75   19.9  0.00926    2.98\n# ℹ 21 more rows\n```\n\n\n:::\n:::\n\n\n\n## Test {.smaller}\n\n-   Hypotheses:\n    -   $H_0$: Errors are $NID(0, \\sigma^2)$\n    -   $H_A$: Errors are not $NID(0, \\sigma^2)$\n\n. . .\n\n-   Visual statistic: Residuals plot (residuals vs. fitted)\n\n. . .\n\n-   Null distributions: Generate residuals from random draws from $N(0, \\hat{sigma}^2)$ using the **nullabor** package\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmethod <- null_lm(log(Volume) ~ log(Girth) + log(Height),\n                  method = \"pboot\")\n```\n:::\n\n\n\n. . .\n\n-   Compare the visual statistic from the data to the null distributions using a **lineup**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2020)\ntrees_lineup <- lineup(method, true = trees_aug, n = 10)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\ndecrypt(\"K4Ul YG3G R8 oSJR3RS8 Nq\")\n```\n\n\n:::\n:::\n\n\n\n## Lineup\n\n::: task\nWhich one is the real residuals plot?\n:::\n\n::: columns\n::: {.column width=\"80%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-visual-inference_files/figure-html/trees-lineup-1.png){fig-align='center' width=960}\n:::\n:::\n\n\n:::\n\n::: {.column width=\"20%\"}\n\n\n```{=html}\n<iframe src=\"https://app.sli.do/event/iJykXLe5CLzhorAiQhjnrg/embed/polls/98b5ff30-7049-4af9-ab7d-f4cd98070cd5\" width=\"300\" height=\"400\"></iframe>\n```\n\n\n:::\n:::\n\n## Decrypt\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndecrypt(\"K4Ul YG3G R8 oSJR3RS8 Nq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"True data in position  3\"\n```\n\n\n:::\n:::\n\n\n\n# The visual inference test\n\n## The visual inference test {.smaller}\n\n-   Notation:\n    -   $n$: number of independent participants\n    -   $X$: number of participants who detect the data plot\n    -   $x$: observed value of $X$\n    -   $m$: number of plots\n    -   $p$: the probability of selecting the data plot\n\n. . .\n\n-   Hypotheses: - $H_0: p = 1 / m$ - The probability of the data plot being selected is $1 / m$ (same as all other plots) - $H_A: p > 1 / m$ - The probability of the data plot being selected is greater than $1 / m$\n\n. . .\n\n-   Under $H_0, X \\sim B(n, 1/m)$. Then, visual inference p-value:\n\n$$ p(X \\ge x) = 1 - P(X \\le x - 1) = \\sum_{k = x}^n {n \\choose k} \\frac{(m - 1)^k}{m^n} $$\n\n-   In R: $P(X \\le x)$ = `pbinom(x, n, p)`\n\n## Calculating p-values\n\nWe did three lineup tests today, where - $x$ is the number of people who spotted the real data and - $n$ is the number of people voting\n\nLet's calculate the p-values\n\n-   Spot the real `mtcars` (n = 20)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pbinom(4, 12, 1/20)\n```\n:::\n\n\n\n-   Spot the real `bdims::hgt` (n = 10)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pbinom(0, 12, 1/10)\n```\n:::\n\n\n\n-   Spot the real residuals of `trees` model (n = 10)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n1 - pbinom(0, 12, 1/20)\n```\n:::\n\n\n\n# Acknowledgements\n\n## Acknowledgements\n\n-   [Statistical inference for exploratory data analysis and model diagnostics](https://royalsocietypublishing.org/doi/10.1098/rsta.2009.0120) by Andreas Buja , Dianne Cook , Heike Hofmann , Michael Lawrence , Eun-Kyung Lee , Deborah F. Swayne, and Hadley Wickham\n-   [Graphical Inference for Infovis](https://vita.had.co.nz/papers/inference-infovis.pdf) by Hadley Wickham, Dianne Cook, Heike Hofmann, and Andreas Buja\n-   [Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly](https://eda.numbat.space/lectures/lecture-11b#2) by Emi Tanaka\n-   [Extending beyond the data, what can and cannot be inferred more generally, given the data collection](https://eda.numbat.space/lectures/lecture-12a#2) by Emi Tanaka\n-   The [**nullabor**](http://dicook.github.io/nullabor/) package\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}